{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77e4d61-d5b9-43b3-8a6c-f26638f169c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#stemming is a process of reducing inflection in words to their root form such as mapping a group of words to the same stem even if the stem itself is not a valid word in the anguage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e57aef-efe2-4bb2-b1f2-362b8366054e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ques 1:- create dataset -> do text-processing -> multi-class classification ->movie data  ->name,discription,jenre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e929de3b-3c38-4c88-bdb5-6feb30fcf322",
   "metadata": {},
   "outputs": [],
   "source": [
    "what is featured extraction from text?--->ML--> input feature is to be given (garbage in garbage out)\n",
    "acqisition->preprocessing-->text vectorisation-->algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad8e1cf-1267-437b-9f27-f9af4285503b",
   "metadata": {},
   "source": [
    "Corpus,vocabulary,document,word\n",
    "One hot coding->problems:\n",
    "sparse array(many 0 in array+overfeeding)\n",
    "out of vocabulary(oov)\n",
    "no fixed size\n",
    "no capturing of semantic meaning\n",
    "\n",
    "bag of words:  people watch campusx write  comment(kitni baar aaye hain(order doesnot matter, context doesnot matter))\n",
    "people watch campusx[1,1,1,0,0]\n",
    "campusx watch campusx[0,1,2,0,0]\n",
    "people write comment[1,0,0,1,1]\n",
    "campusx write comment[0,0,1,1,1]\n",
    "n dimension mein vectoor ko dikhao angle less->similar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "57132ac4-6328-4ddd-8e7e-7ac2d2109e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "855f9658-08ab-4830-8d80-ade44de80b09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>people watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>campusx watch campusx</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>people write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>campusx write comment</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    text  output\n",
       "0   people watch campusx       1\n",
       "1  campusx watch campusx       1\n",
       "2   people write comment       0\n",
       "3  campusx write comment       0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame({'text':['people watch campusx','campusx watch campusx','people write comment','campusx write comment'],'output':[1,1,0,0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e4b90e54-414c-46c3-9c17-9c6287126591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 2, 'watch': 3, 'campusx': 0, 'write': 4, 'comment': 1}\n",
      "[[1 0 1 1 0]]\n",
      "[[0 1 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()\n",
    "bow=cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)\n",
    "print(bow[0].toarray())\n",
    "print(bow[2].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f0d6cf8a-e44c-4815-883c-b965e022bef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.transform([\"campusx watch and write comment of campusx\"]).toarray()#out of vocabulary problem does not exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452383b6-1d50-4f12-9ad5-566bff873e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max-feature\n",
    "disadvantages of bag of words:(spasity,oov,not considering order,bag of word \"this book is good\" or \"this book is not good\" ko ek jaisa hi samjhega angle less hoga\n",
    "and hence meaning bhi ek hi manega------>USE NGRAMS\n",
    "    N-grams----> Bag of n-grams->bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27325fdb-47a1-4677-922b-57a19896324b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people watch': 2, 'watch campusx': 4, 'campusx watch': 0, 'people write': 3, 'write comment': 5, 'campusx write': 1}\n"
     ]
    }
   ],
   "source": [
    "cv.transform([\"campusx watch and write comment of campusx\"]).toarray()#out of vocabulary problem does not exist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(ngram_range=(2,2))\n",
    "bow=cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3a288cae-03d4-4e94-bfb3-b6c17e96252b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'people': 6, 'watch': 11, 'campusx': 0, 'people watch': 7, 'watch campusx': 12, 'people watch campusx': 8, 'campusx watch': 1, 'campusx watch campusx': 2, 'write': 13, 'comment': 5, 'people write': 9, 'write comment': 14, 'people write comment': 10, 'campusx write': 3, 'campusx write comment': 4}\n",
      "15\n"
     ]
    }
   ],
   "source": [
    "cv.transform([\"campusx watch and write comment of campusx\"]).toarray()#out of vocabulary problem does not exist\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(ngram_range=(1,3))\n",
    "bow=cv.fit_transform(df['text'])\n",
    "print(cv.vocabulary_)\n",
    "print(len(cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b015fa9-87ec-4eab-b000-4c54533eb172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#benifits of N-grams ---> able to capture semantic of the sentence  2. easy to implemrnt\n",
    "#disadvatages of N-grams-->Dimention increases  2. slow down algo   3. ignore new words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929462c2-acc9-4878-ba34-76afffd1e41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# agar ek word ek paticular document(line) mein jyada aa rha hai pr baki corpus mein nhi aa rha hai --> to uss word ko jyada priority denge\n",
    "#tf=gives different values to different words\n",
    "#TF(t,d)=number of occurrences of term t in document d/total number of term in the document d\n",
    "##IDF(t)=Total number of documents in the corpus/nnumber of documents wth the term t in them\n",
    "#term rare(IDF high)   term frequent (IDF low)\n",
    "#koi bhi term kitna frequent hai kisi bhi document mein* term kitna rare hai poore corpus mein\n",
    "# TF*IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8ead245-5559-4e8a-ae84-6ac0de4e9df4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 3526: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Open the file and process its contents\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTop_rated.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m---> 11\u001b[0m     text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Use the loaded NLP model to process the text\u001b[39;00m\n\u001b[0;32m     14\u001b[0m doc \u001b[38;5;241m=\u001b[39m nlp(text)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\encodings\\cp1252.py:23\u001b[0m, in \u001b[0;36mIncrementalDecoder.decode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m, final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m codecs\u001b[38;5;241m.\u001b[39mcharmap_decode(\u001b[38;5;28minput\u001b[39m,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,decoding_table)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 3526: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d3b536-2e58-44ec-915d-b7bb1a85b50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "with open(\"Top_rated.txt\",\"r\") as file:\n",
    "    text=file.read()\n",
    "doc=nlp(text)\n",
    "print(\"Tokens:\")\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
